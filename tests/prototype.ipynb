{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "LANGUAGE_MODEL = 'tinyllama:latest'\n",
    "EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "with open('knot theory/ambitious_antrepreneur.txt', 'r') as file:\n",
    "    dataset = file.read()\n",
    "    print(f'Loaded {len(dataset)} characters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() + '\\n'\n",
    "    return text\n",
    "\n",
    "celtic_knots_pdf = 'knot theory/celtic_knots.pdf'\n",
    "protiens_pdf = 'knot theory/protiens.pdf'\n",
    "quantum_computing_pdf = 'knot theory/quantum_computing.pdf'\n",
    "\n",
    "celtic_knots_text = extract_text_from_pdf(celtic_knots_pdf)\n",
    "protiens_text = extract_text_from_pdf(protiens_pdf)\n",
    "quantum_computing_text = extract_text_from_pdf(quantum_computing_pdf)\n",
    "\n",
    "dataset = [celtic_knots_text, protiens_text, quantum_computing_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Chunk Embeddings\n",
    "vector_db = []\n",
    "\n",
    "def add_chunk_to_db(chunk):\n",
    "    embedding = ollama.embed(EMBEDDING_MODEL, chunk)['embeddings'][0]\n",
    "    vector_db.append((chunk, embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to chunks and add to db\n",
    "def split_to_chunks(dataset):\n",
    "    for document in dataset:\n",
    "        chunks = document.split('.')\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            if chunk:\n",
    "                add_chunk_to_db(chunk)\n",
    "        print(f'Added {i+1} chunks to the db.')\n",
    "\n",
    "split_to_chunks(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cosine similarity\n",
    "def cosine_similarity(a, b):\n",
    "    dot_product = sum([x * y for x, y in zip(a, b)])\n",
    "    norm_a = sum([x ** 2 for x in a]) ** 0.5\n",
    "    norm_b = sum([x ** 2 for x in b]) ** 0.5\n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Knowledge Retrieval\n",
    "def retriev(input_query, top_n=3):\n",
    "    embedded_query = ollama.embed(LANGUAGE_MODEL, input_query)['embeddings'][0]\n",
    "    similarities = []\n",
    "    for chunk, embedding in vector_db:\n",
    "        similarity = cosine_similarity(embedded_query, embedding)\n",
    "        similarities.append((chunk, similarity))\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = input('\\nSend a message: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_knowledge = retriev(user_prompt, 5)\n",
    "print('Retrieved knowledge:')\n",
    "for chunk, similarity in retrieved_knowledge:\n",
    "    print(f' - (similarity: {similarity:.2f}) {chunk}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define System Prompt\n",
    "system_prompt = \"\"\"You are a helpful and knowledgeable AI assistant. Your primary goal is to assist users effectively based on the information provided to you.\n",
    "\n",
    "You have access to external documents which serve as your knowledge base. Use the following retrieved information to answer the user's question accurately and comprehensively:\n",
    "{retrieved_knowledge_formatted}\n",
    "\n",
    "Please ensure your responses are:\n",
    "- Accurate and factually correct based on the provided documents.\n",
    "- Relevant to the user's query.\n",
    "- Concise and to the point, avoiding unnecessary jargon.\n",
    "- Helpful and address the user's needs.\n",
    "\n",
    "If the retrieved information is insufficient to answer the user's question, acknowledge this and state that you cannot provide a complete answer at this time.\n",
    "\"\"\"\n",
    "\n",
    "retrieved_knowledge_formatted = '\\n'.join([f' - {chunk}' for chunk, similarity in retrieved_knowledge])\n",
    "system_prompt = system_prompt.format(retrieved_knowledge_formatted=retrieved_knowledge_formatted)\n",
    "\n",
    "master_prompt = [\n",
    "    {'role': 'system', 'content': system_prompt},\n",
    "    {'role': user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Model's Response\n",
    "stream = ollama.chat(\n",
    "    model='tinyllama:latest', \n",
    "    messages=[\n",
    "        {'role': 'user', 'content': user_prompt}\n",
    "    ], \n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk['message']['content'], end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
